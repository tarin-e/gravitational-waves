{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# signal processing\n",
    "from scipy import signal\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.use_deterministic_algorithms(True) # Needed for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wifi-staff-172-24-33-52.net.auckland.ac.nz\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "myHostName = socket.gethostname()\n",
    "print(myHostName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "# number of channels the signal has\n",
    "nc = 1\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# number of signals per iteration\n",
    "batch_size = 32\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GWDataset(Dataset):\n",
    "    def __init__(self, signals_csv, parameters_csv):\n",
    "        self.parameters = pd.read_csv(parameters_csv)\n",
    "        # choose only the most important parameters for parameter estimation\n",
    "        # self.parameters = self.parameters[['beta1_IC_b', 'A(km)', 'omega_0(rad|s)']].astype('float32')\n",
    "        # remove erroneous parameters\n",
    "        keep_signals_idx = np.array(self.parameters[self.parameters['beta1_IC_b'] > 0].index)\n",
    "        self.parameters = self.parameters.iloc[keep_signals_idx,:]\n",
    "        self.parameters['beta1_IC_b_bins'] = pd.qcut(self.parameters['beta1_IC_b'], q=10, labels=False)\n",
    "        self.parameters = self.parameters[['beta1_IC_b_bins']].astype('float32')\n",
    "\n",
    "        # convert to numpy array\n",
    "        self.parameters = self.parameters.values\n",
    "        self.original_parameters = self.parameters\n",
    "        self.augmented_parameters = np.empty(shape = (0, self.parameters.shape[1]))\n",
    "\n",
    "        # self.data = pd.read_csv(signals_csv).astype('float32')\n",
    "        # drop corresponding signals which have erroneous parameter values\n",
    "        self.data = self.data.iloc[:,keep_signals_idx]\n",
    "        self.data = self.data.values\n",
    "        self.original_data = self.data\n",
    "        self.augmented_data = np.empty(shape = (self.data.shape[0], 0))\n",
    "\n",
    "\n",
    "    def calc_stats(self):\n",
    "        self.mean = self.data.mean()\n",
    "        print('Dataset mean: ',  self.mean)\n",
    "        self.std = np.std(self.data, axis=None)\n",
    "        print('Dataset std: ',  self.std)\n",
    "        self.scaling_factor = 5\n",
    "        print('Dataset scaling factor (to match noise in generator): ',  self.scaling_factor)\n",
    "\n",
    "    def get_common(self):\n",
    "        self.common_ylim_signal = (self.data[:,:].min(), self.data[:,:].max())\n",
    "        return self.common_ylim_signal\n",
    "    \n",
    "    def standardize(self, signal):\n",
    "        standardized_signal = (signal - self.mean) / self.std\n",
    "        standardized_signal = standardized_signal / self.scaling_factor\n",
    "        return standardized_signal\n",
    "\n",
    "    def shift_augmentation(self, signal):\n",
    "        shift = np.random.normal(0, 25, 1)\n",
    "        shifted_signal = np.roll(signal, int(shift))\n",
    "        \n",
    "        return shifted_signal\n",
    "\n",
    "    def scale_augmentation(self, signal):\n",
    "        scale_factor = np.random.normal(10, 0.2, 1)\n",
    "        scale_factor = np.maximum(scale_factor, 0)\n",
    "        scaled_signal = scale_factor * signal\n",
    "        return scaled_signal\n",
    "\n",
    "    def jittering_augmentation(self, signal):\n",
    "        # todo: add noise only after time of core bounce\n",
    "        # noise_start_time = 203\n",
    "        noise = np.random.normal(0, 1, signal.shape[1])\n",
    "        jittered_signal = signal + noise\n",
    "    \n",
    "        return jittered_signal\n",
    "\n",
    "    def mixture_augmentation(self, signal_1, signal_2):\n",
    "        distance_multiplier = np.random.normal(0.5, 0.2, 1)\n",
    "        # clip signal to range [0,1] as this is the multiplier by the normalised difference in signals\n",
    "        distance_multiplier = np.clip(distance_multiplier, 0, 1)\n",
    "        mixture_signal = signal_1 + distance_multiplier * (signal_2 - signal_1)\n",
    "\n",
    "        return mixture_signal\n",
    "\n",
    "    def window_warping_augmentation(self, signal):\n",
    "        # take window size of 10% of the signal with a warping factor of 2 or 0.5 (from literature)\n",
    "        warping_factor =  random.choice([0.5, 2])\n",
    "        # warping_factor = 0.5\n",
    "\n",
    "        window_size = math.floor(signal.shape[1] / 10)\n",
    "        scaled_window_size = warping_factor * window_size\n",
    "\n",
    "        # don't warp anything a little bit before the core-bounce - preserves core-bounce position\n",
    "        window_min_idx = 203\n",
    "\n",
    "        # find random reference position for start of window warping\n",
    "        window_start_idx = np.random.randint(window_min_idx, signal.shape[1] - scaled_window_size*2)\n",
    "        window_end_idx = window_start_idx + window_size\n",
    "\n",
    "        # select between warping by factor 1/2 or 2\n",
    "        if (warping_factor == 2):\n",
    "            # extract values before, at and after the window\n",
    "            # clip end of signal to make up for extra size due to window warping\n",
    "            signal_before_window = signal[0][:window_start_idx]\n",
    "            signal_window = signal[0][window_start_idx:window_end_idx]\n",
    "            signal_after_window = signal[0][window_end_idx:int(signal.shape[1]-(window_size))]\n",
    "            # print(signal_after_window)\n",
    "\n",
    "            # time points\n",
    "            t = np.arange(len(signal_window))\n",
    "            warped_t = np.arange(0, len(signal_window), 0.5)\n",
    "\n",
    "            # interpolation for window warping\n",
    "            signal_window_warped = np.interp(warped_t, t, signal_window)\n",
    "\n",
    "            # combine signals\n",
    "            warped_signal = np.concatenate((signal_before_window, signal_window_warped, signal_after_window), axis=0)\n",
    "        elif (warping_factor == 0.5):\n",
    "            # extract values before, at and after the window\n",
    "            # clip end of signal to make up for extra size due to window warping\n",
    "            signal_before_window = signal[0][:window_start_idx]\n",
    "            signal_window = signal[0][window_start_idx:window_end_idx]\n",
    "            signal_after_window = signal[0][window_end_idx:]\n",
    "            # add values to end of signal to make up for downsampled window\n",
    "            signal_after_window = np.pad(signal_after_window, (0, int(window_size - scaled_window_size)), mode='edge')\n",
    "\n",
    "            signal_window_warped = signal_window[::int(1/warping_factor)]\n",
    "\n",
    "            warped_signal = np.concatenate((signal_before_window, signal_window_warped, signal_after_window), axis=0)\n",
    "        else:\n",
    "            warped_signal = signal\n",
    "\n",
    "        return warped_signal\n",
    "\n",
    "    def augmentation(self, desired_augmented_data_count):\n",
    "        while self.data.shape[1] < desired_augmented_data_count:\n",
    "            idx_1 = np.random.randint(0, self.data.shape[1])\n",
    "            signal_1 = self.data[:, idx_1]\n",
    "            signal_1 = signal_1.reshape(1, -1)\n",
    "\n",
    "            # second signal used only used for mixture signals data augmentation\n",
    "            # idx_2 = np.random.choice([x for x in range(0, self.data.shape[1]) if x != idx_1])\n",
    "            # signal_2 = self.data[:, idx_2]\n",
    "            # signal_2 = signal_2.reshape(1, -1)\n",
    "\n",
    "            # call selected augmentation function here\n",
    "            augmented_signal = self.window_warping_augmentation(signal_1)\n",
    "            # augmented_signal = self.mixture_augmentation(signal_1, signal_2)\n",
    "\n",
    "            self.augmented_data = np.insert(self.augmented_data, self.augmented_data.shape[1], augmented_signal, axis=1)\n",
    "            self.data = np.insert(self.data, self.data.shape[1], augmented_signal, axis=1)\n",
    "\n",
    "            # just sample parameters for now, haven't figured out a way to augment them to make consistent with signal augmentation\n",
    "            augmented_parameter = self.parameters[idx_1, :]\n",
    "            augmented_parameter = augmented_parameter.reshape(1, -1)\n",
    "\n",
    "            self.augmented_parameters = np.insert(self.augmented_parameters, self.augmented_parameters.shape[0], augmented_parameter, axis=0)\n",
    "            self.parameters = np.insert(self.parameters, self.parameters.shape[0], augmented_parameter, axis=0)\n",
    "\n",
    "        print(self.data.shape)\n",
    "        print(self.parameters.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signal = self.data[:, idx]\n",
    "        signal = signal.reshape(1, -1)\n",
    "\n",
    "        parameter = self.parameters[idx,:]\n",
    "        parameter = parameter.reshape(1, -1)\n",
    "\n",
    "        # pad signals with edge values to be length 512\n",
    "        desired_length = 512\n",
    "\n",
    "        padding_left = (desired_length - len(signal[0])) // 2\n",
    "        padding_right = desired_length - len(signal[0]) - padding_left\n",
    "\n",
    "        signal_padded = np.pad(signal, ((0, 0), (padding_left, padding_right)), mode='edge')\n",
    "\n",
    "        signal_standardized = self.standardize(signal_padded)\n",
    "\n",
    "        return signal_standardized, parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [407, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/tarineccleston/Documents/software-ds/gravitational-waves/gw-classification/gw_classification.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tarineccleston/Documents/software-ds/gravitational-waves/gw-classification/gw_classification.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tarineccleston/Documents/software-ds/gravitational-waves/gw-classification/gw_classification.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Split the data into training and testing sets (80% train, 20% test)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tarineccleston/Documents/software-ds/gravitational-waves/gw-classification/gw_classification.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m x_train, x_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(x, y, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tarineccleston/Documents/software-ds/gravitational-waves/gw-classification/gw_classification.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# train_data = HARData(X_train, y_train)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tarineccleston/Documents/software-ds/gravitational-waves/gw-classification/gw_classification.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# test_data = HARData(X_test, y_test)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tarineccleston/Documents/software-ds/gravitational-waves/gw-classification/gw_classification.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tarineccleston/Documents/software-ds/gravitational-waves/gw-classification/gw_classification.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tarineccleston/Documents/software-ds/gravitational-waves/gw-classification/gw_classification.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/software-ds/gravitational-waves/gw-env/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/software-ds/gravitational-waves/gw-env/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2646\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2643\u001b[0m \u001b[39mif\u001b[39;00m n_arrays \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2644\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAt least one array required as input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2646\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39;49marrays)\n\u001b[1;32m   2648\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[1;32m   2649\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2650\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m\n\u001b[1;32m   2651\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/software-ds/gravitational-waves/gw-env/lib/python3.9/site-packages/sklearn/utils/validation.py:453\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \n\u001b[1;32m    436\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    452\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[0;32m--> 453\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[1;32m    454\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/software-ds/gravitational-waves/gw-env/lib/python3.9/site-packages/sklearn/utils/validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    405\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    410\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [407, 1]"
     ]
    }
   ],
   "source": [
    "x = pd.read_csv(\"../data/gw-raw-data/richers_1764.csv\")\n",
    "y = pd.read_csv(\"../data/gw-raw-data/richers_1764_parameters.csv\")\n",
    "\n",
    "# remove erroneous signals and select only beta_IC_b as label\n",
    "keep_signals_idx = np.array(y[y['beta1_IC_b'] > 0].index)\n",
    "y = y.iloc[keep_signals_idx,:]\n",
    "# bin labels to get discretised data\n",
    "y['beta1_IC_b_bins'] = pd.qcut(y['beta1_IC_b'], q=10, labels=False)\n",
    "y = y[['beta1_IC_b_bins']].astype('float32')\n",
    "# convert to numpy array\n",
    "y = y.values\n",
    "# y = np.transpose(y)\n",
    "\n",
    "# drop corresponding signals which have erroneous parameter values\n",
    "x = x.iloc[:,keep_signals_idx]\n",
    "x = x.values\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train_data = HARData(X_train, y_train)\n",
    "# test_data = HARData(X_test, y_test)\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gw-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
