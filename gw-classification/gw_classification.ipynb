{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# signal processing\n",
    "from scipy import signal\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.use_deterministic_algorithms(True) # Needed for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wifi-staff-172-24-33-52.net.auckland.ac.nz\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "myHostName = socket.gethostname()\n",
    "print(myHostName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "# number of channels the signal has\n",
    "nc = 1\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# number of signals per iteration\n",
    "batch_size = 32\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GWDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        # convert to numpy array\n",
    "        self.original_parameters = y\n",
    "        self.augmented_parameters = np.empty(shape = (0, self.parameters.shape[1]))\n",
    "        x = np.transpose(x)\n",
    "        self.original_data = x\n",
    "        self.augmented_data = np.empty(shape = (self.data.shape[0], 0))\n",
    "\n",
    "    def calc_stats(self):\n",
    "        self.mean = self.data.mean()\n",
    "        print('Dataset mean: ',  self.mean)\n",
    "        self.std = np.std(self.data, axis=None)\n",
    "        print('Dataset std: ',  self.std)\n",
    "        self.scaling_factor = 5\n",
    "        print('Dataset scaling factor (to match noise in generator): ',  self.scaling_factor)\n",
    "\n",
    "    def get_common(self):\n",
    "        self.common_ylim_signal = (self.data[:,:].min(), self.data[:,:].max())\n",
    "        return self.common_ylim_signal\n",
    "    \n",
    "    def standardize(self, signal):\n",
    "        standardized_signal = (signal - self.mean) / self.std\n",
    "        standardized_signal = standardized_signal / self.scaling_factor\n",
    "        return standardized_signal\n",
    "\n",
    "    def shift_augmentation(self, signal):\n",
    "        shift = np.random.normal(0, 25, 1)\n",
    "        shifted_signal = np.roll(signal, int(shift))\n",
    "        \n",
    "        return shifted_signal\n",
    "\n",
    "    def scale_augmentation(self, signal):\n",
    "        scale_factor = np.random.normal(10, 0.2, 1)\n",
    "        scale_factor = np.maximum(scale_factor, 0)\n",
    "        scaled_signal = scale_factor * signal\n",
    "        return scaled_signal\n",
    "\n",
    "    def jittering_augmentation(self, signal):\n",
    "        # todo: add noise only after time of core bounce\n",
    "        # noise_start_time = 203\n",
    "        noise = np.random.normal(0, 1, signal.shape[1])\n",
    "        jittered_signal = signal + noise\n",
    "    \n",
    "        return jittered_signal\n",
    "\n",
    "    def mixture_augmentation(self, signal_1, signal_2):\n",
    "        distance_multiplier = np.random.normal(0.5, 0.2, 1)\n",
    "        # clip signal to range [0,1] as this is the multiplier by the normalised difference in signals\n",
    "        distance_multiplier = np.clip(distance_multiplier, 0, 1)\n",
    "        mixture_signal = signal_1 + distance_multiplier * (signal_2 - signal_1)\n",
    "\n",
    "        return mixture_signal\n",
    "\n",
    "    def window_warping_augmentation(self, signal):\n",
    "        # take window size of 10% of the signal with a warping factor of 2 or 0.5 (from literature)\n",
    "        warping_factor =  random.choice([0.5, 2])\n",
    "        # warping_factor = 0.5\n",
    "\n",
    "        window_size = math.floor(signal.shape[1] / 10)\n",
    "        scaled_window_size = warping_factor * window_size\n",
    "\n",
    "        # don't warp anything a little bit before the core-bounce - preserves core-bounce position\n",
    "        window_min_idx = 203\n",
    "\n",
    "        # find random reference position for start of window warping\n",
    "        window_start_idx = np.random.randint(window_min_idx, signal.shape[1] - scaled_window_size*2)\n",
    "        window_end_idx = window_start_idx + window_size\n",
    "\n",
    "        # select between warping by factor 1/2 or 2\n",
    "        if (warping_factor == 2):\n",
    "            # extract values before, at and after the window\n",
    "            # clip end of signal to make up for extra size due to window warping\n",
    "            signal_before_window = signal[0][:window_start_idx]\n",
    "            signal_window = signal[0][window_start_idx:window_end_idx]\n",
    "            signal_after_window = signal[0][window_end_idx:int(signal.shape[1]-(window_size))]\n",
    "            # print(signal_after_window)\n",
    "\n",
    "            # time points\n",
    "            t = np.arange(len(signal_window))\n",
    "            warped_t = np.arange(0, len(signal_window), 0.5)\n",
    "\n",
    "            # interpolation for window warping\n",
    "            signal_window_warped = np.interp(warped_t, t, signal_window)\n",
    "\n",
    "            # combine signals\n",
    "            warped_signal = np.concatenate((signal_before_window, signal_window_warped, signal_after_window), axis=0)\n",
    "        elif (warping_factor == 0.5):\n",
    "            # extract values before, at and after the window\n",
    "            # clip end of signal to make up for extra size due to window warping\n",
    "            signal_before_window = signal[0][:window_start_idx]\n",
    "            signal_window = signal[0][window_start_idx:window_end_idx]\n",
    "            signal_after_window = signal[0][window_end_idx:]\n",
    "            # add values to end of signal to make up for downsampled window\n",
    "            signal_after_window = np.pad(signal_after_window, (0, int(window_size - scaled_window_size)), mode='edge')\n",
    "\n",
    "            signal_window_warped = signal_window[::int(1/warping_factor)]\n",
    "\n",
    "            warped_signal = np.concatenate((signal_before_window, signal_window_warped, signal_after_window), axis=0)\n",
    "        else:\n",
    "            warped_signal = signal\n",
    "\n",
    "        return warped_signal\n",
    "\n",
    "    def augmentation(self, desired_augmented_data_count):\n",
    "        while self.data.shape[1] < desired_augmented_data_count:\n",
    "            idx_1 = np.random.randint(0, self.data.shape[1])\n",
    "            signal_1 = self.data[:, idx_1]\n",
    "            signal_1 = signal_1.reshape(1, -1)\n",
    "\n",
    "            # second signal used only used for mixture signals data augmentation\n",
    "            # idx_2 = np.random.choice([x for x in range(0, self.data.shape[1]) if x != idx_1])\n",
    "            # signal_2 = self.data[:, idx_2]\n",
    "            # signal_2 = signal_2.reshape(1, -1)\n",
    "\n",
    "            # call selected augmentation function here\n",
    "            augmented_signal = self.window_warping_augmentation(signal_1)\n",
    "            # augmented_signal = self.mixture_augmentation(signal_1, signal_2)\n",
    "\n",
    "            self.augmented_data = np.insert(self.augmented_data, self.augmented_data.shape[1], augmented_signal, axis=1)\n",
    "            self.data = np.insert(self.data, self.data.shape[1], augmented_signal, axis=1)\n",
    "\n",
    "            # just sample parameters for now, haven't figured out a way to augment them to make consistent with signal augmentation\n",
    "            augmented_parameter = self.parameters[idx_1, :]\n",
    "            augmented_parameter = augmented_parameter.reshape(1, -1)\n",
    "\n",
    "            self.augmented_parameters = np.insert(self.augmented_parameters, self.augmented_parameters.shape[0], augmented_parameter, axis=0)\n",
    "            self.parameters = np.insert(self.parameters, self.parameters.shape[0], augmented_parameter, axis=0)\n",
    "\n",
    "        print(self.data.shape)\n",
    "        print(self.parameters.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signal = self.data[:, idx]\n",
    "        signal = signal.reshape(1, -1)\n",
    "\n",
    "        parameter = self.parameters[idx,:]\n",
    "        parameter = parameter.reshape(1, -1)\n",
    "\n",
    "        # pad signals with edge values to be length 512\n",
    "        desired_length = 512\n",
    "\n",
    "        padding_left = (desired_length - len(signal[0])) // 2\n",
    "        padding_right = desired_length - len(signal[0]) - padding_left\n",
    "\n",
    "        signal_padded = np.pad(signal, ((0, 0), (padding_left, padding_right)), mode='edge')\n",
    "\n",
    "        signal_standardized = self.standardize(signal_padded)\n",
    "\n",
    "        return signal_standardized, parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(\"../data/gw-raw-data/richers_1764.csv\")\n",
    "y = pd.read_csv(\"../data/gw-raw-data/richers_1764_parameters.csv\")\n",
    "\n",
    "# remove erroneous signals and select only beta_IC_b as label\n",
    "keep_signals_idx = np.array(y[y['beta1_IC_b'] > 0].index)\n",
    "y = y.iloc[keep_signals_idx,:]\n",
    "# bin labels to get discretised data\n",
    "y['beta1_IC_b_bins'] = pd.qcut(y['beta1_IC_b'], q=10, labels=False)\n",
    "y = y[['beta1_IC_b_bins']].astype('float32')\n",
    "# convert to numpy array\n",
    "y = y.values\n",
    "\n",
    "# drop corresponding signals which have erroneous parameter values\n",
    "x = x.iloc[:,keep_signals_idx]\n",
    "x = x.values\n",
    "\n",
    "# only transpore x due to compatibility issues with train_test_split\n",
    "x = np.transpose(x)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=manualSeed)\n",
    "\n",
    "train_data = GWDataset(x_train, y_train)\n",
    "test_data = GWDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gw-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
