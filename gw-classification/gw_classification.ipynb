{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# signal processing\n",
    "from scipy import signal\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.use_deterministic_algorithms(True) # Needed for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarins-MacBook.local\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "myHostName = socket.gethostname()\n",
    "print(myHostName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "# number of channels the signal has\n",
    "nc = 1\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# number of signals per iteration\n",
    "batch_size = 32\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GWDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        # convert to numpy array\n",
    "        self.original_parameters = y\n",
    "        self.parameters = self.original_parameters\n",
    "        self.augmented_parameters = np.empty(shape = (0, self.parameters.shape[1]))\n",
    "\n",
    "        x = np.transpose(x)\n",
    "        self.original_data = x\n",
    "        self.data = self.original_data\n",
    "        self.augmented_data = np.empty(shape = (self.data.shape[0], 0))\n",
    "\n",
    "    def calc_stats(self):\n",
    "        self.mean = self.data.mean()\n",
    "        print('Dataset mean: ',  self.mean)\n",
    "        self.std = np.std(self.data, axis=None)\n",
    "        print('Dataset std: ',  self.std)\n",
    "        self.scaling_factor = 5\n",
    "        print('Dataset scaling factor (to match noise in generator): ',  self.scaling_factor)\n",
    "\n",
    "    def get_common(self):\n",
    "        self.common_ylim_signal = (self.data[:,:].min(), self.data[:,:].max())\n",
    "        return self.common_ylim_signal\n",
    "    \n",
    "    def standardize(self, signal):\n",
    "        standardized_signal = (signal - self.mean) / self.std\n",
    "        standardized_signal = standardized_signal / self.scaling_factor\n",
    "        return standardized_signal\n",
    "\n",
    "    def shift_augmentation(self, signal):\n",
    "        shift = np.random.normal(0, 25, 1)\n",
    "        shifted_signal = np.roll(signal, int(shift))\n",
    "        \n",
    "        return shifted_signal\n",
    "\n",
    "    def scale_augmentation(self, signal):\n",
    "        scale_factor = np.random.normal(10, 0.2, 1)\n",
    "        scale_factor = np.maximum(scale_factor, 0)\n",
    "        scaled_signal = scale_factor * signal\n",
    "        return scaled_signal\n",
    "\n",
    "    def jittering_augmentation(self, signal):\n",
    "        # todo: add noise only after time of core bounce\n",
    "        # noise_start_time = 203\n",
    "        noise = np.random.normal(0, 1, signal.shape[1])\n",
    "        jittered_signal = signal + noise\n",
    "    \n",
    "        return jittered_signal\n",
    "\n",
    "    def mixture_augmentation(self, signal_1, signal_2):\n",
    "        distance_multiplier = np.random.normal(0.5, 0.2, 1)\n",
    "        # clip signal to range [0,1] as this is the multiplier by the normalised difference in signals\n",
    "        distance_multiplier = np.clip(distance_multiplier, 0, 1)\n",
    "        mixture_signal = signal_1 + distance_multiplier * (signal_2 - signal_1)\n",
    "\n",
    "        return mixture_signal\n",
    "\n",
    "    def window_warping_augmentation(self, signal):\n",
    "        # take window size of 10% of the signal with a warping factor of 2 or 0.5 (from literature)\n",
    "        warping_factor =  random.choice([0.5, 2])\n",
    "        # warping_factor = 0.5\n",
    "\n",
    "        window_size = math.floor(signal.shape[1] / 10)\n",
    "        scaled_window_size = warping_factor * window_size\n",
    "\n",
    "        # don't warp anything a little bit before the core-bounce - preserves core-bounce position\n",
    "        window_min_idx = 203\n",
    "\n",
    "        # find random reference position for start of window warping\n",
    "        window_start_idx = np.random.randint(window_min_idx, signal.shape[1] - scaled_window_size*2)\n",
    "        window_end_idx = window_start_idx + window_size\n",
    "\n",
    "        # select between warping by factor 1/2 or 2\n",
    "        if (warping_factor == 2):\n",
    "            # extract values before, at and after the window\n",
    "            # clip end of signal to make up for extra size due to window warping\n",
    "            signal_before_window = signal[0][:window_start_idx]\n",
    "            signal_window = signal[0][window_start_idx:window_end_idx]\n",
    "            signal_after_window = signal[0][window_end_idx:int(signal.shape[1]-(window_size))]\n",
    "\n",
    "            # time points\n",
    "            t = np.arange(len(signal_window))\n",
    "            warped_t = np.arange(0, len(signal_window), 0.5)\n",
    "\n",
    "            # interpolation for window warping\n",
    "            signal_window_warped = np.interp(warped_t, t, signal_window)\n",
    "\n",
    "            # combine signals\n",
    "            warped_signal = np.concatenate((signal_before_window, signal_window_warped, signal_after_window), axis=0)\n",
    "        elif (warping_factor == 0.5):\n",
    "            # extract values before, at and after the window\n",
    "            # clip end of signal to make up for extra size due to window warping\n",
    "            signal_before_window = signal[0][:window_start_idx]\n",
    "            signal_window = signal[0][window_start_idx:window_end_idx]\n",
    "            signal_after_window = signal[0][window_end_idx:]\n",
    "            # add values to end of signal to make up for downsampled window\n",
    "            signal_after_window = np.pad(signal_after_window, (0, int(window_size - scaled_window_size)), mode='edge')\n",
    "\n",
    "            signal_window_warped = signal_window[::int(1/warping_factor)]\n",
    "\n",
    "            warped_signal = np.concatenate((signal_before_window, signal_window_warped, signal_after_window), axis=0)\n",
    "        else:\n",
    "            warped_signal = signal\n",
    "\n",
    "        return warped_signal\n",
    "\n",
    "    def augmentation(self, desired_augmented_data_count):\n",
    "        while self.data.shape[1] < desired_augmented_data_count:\n",
    "            idx_1 = np.random.randint(0, self.data.shape[1])\n",
    "            signal_1 = self.data[:, idx_1]\n",
    "            signal_1 = signal_1.reshape(1, -1)\n",
    "\n",
    "            # second signal used only used for mixture signals data augmentation\n",
    "            # idx_2 = np.random.choice([x for x in range(0, self.data.shape[1]) if x != idx_1])\n",
    "            # signal_2 = self.data[:, idx_2]\n",
    "            # signal_2 = signal_2.reshape(1, -1)\n",
    "\n",
    "            # call selected augmentation function here\n",
    "            augmented_signal = self.window_warping_augmentation(signal_1)\n",
    "            # augmented_signal = self.mixture_augmentation(signal_1, signal_2)\n",
    "\n",
    "            self.augmented_data = np.insert(self.augmented_data, self.augmented_data.shape[1], augmented_signal, axis=1)\n",
    "            self.data = np.insert(self.data, self.data.shape[1], augmented_signal, axis=1)\n",
    "\n",
    "            # just sample parameters for now, haven't figured out a way to augment them to make consistent with signal augmentation\n",
    "            augmented_parameter = self.parameters[idx_1, :]\n",
    "            augmented_parameter = augmented_parameter.reshape(1, -1)\n",
    "\n",
    "            self.augmented_parameters = np.insert(self.augmented_parameters, self.augmented_parameters.shape[0], augmented_parameter, axis=0)\n",
    "            self.parameters = np.insert(self.parameters, self.parameters.shape[0], augmented_parameter, axis=0)\n",
    "\n",
    "        print(self.data.shape)\n",
    "        print(self.parameters.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signal = self.data[:, idx]\n",
    "        signal = signal.reshape(1, -1)\n",
    "\n",
    "        parameter = self.parameters[idx,:]\n",
    "        parameter = parameter.reshape(1, -1)\n",
    "\n",
    "        # pad signals with edge values to be length 512\n",
    "        desired_length = 512\n",
    "\n",
    "        padding_left = (desired_length - len(signal[0])) // 2\n",
    "        padding_right = desired_length - len(signal[0]) - padding_left\n",
    "\n",
    "        signal_padded = np.pad(signal, ((0, 0), (padding_left, padding_right)), mode='edge')\n",
    "\n",
    "        signal_standardized = self.standardize(signal_padded)\n",
    "\n",
    "        return signal_standardized, parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset mean:  -0.037543874\n",
      "Dataset std:  31.566572\n",
      "Dataset scaling factor (to match noise in generator):  5\n",
      "Dataset mean:  -0.08422502\n",
      "Dataset std:  31.330341\n",
      "Dataset scaling factor (to match noise in generator):  5\n"
     ]
    }
   ],
   "source": [
    "x = pd.read_csv(\"../data/gw-raw-data/richers_1764.csv\")\n",
    "y = pd.read_csv(\"../data/gw-raw-data/richers_1764_parameters.csv\")\n",
    "\n",
    "# remove erroneous signals and select only beta_IC_b as label\n",
    "keep_signals_idx = np.array(y[y['beta1_IC_b'] > 0].index)\n",
    "y = y.iloc[keep_signals_idx,:]\n",
    "# select continuous beta_IC_b values for now\n",
    "y = y[['beta1_IC_b']].astype('float32')\n",
    "# bin labels to get discretised data\n",
    "# y['beta1_IC_b_bins'] = pd.qcut(y['beta1_IC_b'], q=10, labels=False)\n",
    "# y = y[['beta1_IC_b_bins']].astype('float32')\n",
    "# convert to numpy array\n",
    "y = y.values\n",
    "\n",
    "# drop corresponding signals which have erroneous parameter values\n",
    "x = x.iloc[:,keep_signals_idx]\n",
    "x = x.values.astype('float32')\n",
    "\n",
    "# only transpore x due to compatibility issues with train_test_split\n",
    "x = np.transpose(x)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=manualSeed)\n",
    "\n",
    "train_data = GWDataset(x_train, y_train)\n",
    "test_data = GWDataset(x_test, y_test)\n",
    "\n",
    "# split the total augmentated signal count to 6000 to match distribution of GAN training dataset\n",
    "# train_data.augmentation(4800)\n",
    "# test_data.augmentation(1200)\n",
    "\n",
    "train_data.calc_stats()\n",
    "test_data.calc_stats()\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_signals_batch, real_parameter_batch  = next(iter(train_loader))\n",
    "\n",
    "common_ylim = (real_signals_batch[:,:,:].min(), real_signals_batch[:,:,:].max())\n",
    "parameter_names = ['beta1_IC_b']\n",
    "\n",
    "def plot_waveforms(real_signals_batch, real_parameter_batch):\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Plot each signal on a separate subplot\n",
    "    for i, ax in enumerate(axes):\n",
    "        x = np.arange(real_signals_batch.size(dim=2))\n",
    "        y = real_signals_batch[i, :, :].flatten()\n",
    "        ax.plot(x, y)\n",
    "\n",
    "        ax.axvline(x=256, color='black', linestyle='--', alpha=0.5)\n",
    "        ax.set_title(f'Signal {i + 1}')\n",
    "        ax.grid(True)\n",
    "        ax.set_ylim(common_ylim)\n",
    "\n",
    "        # Get parameter values as a NumPy array\n",
    "        parameters = real_parameter_batch[i, :].numpy()[0]\n",
    "\n",
    "        # Combine parameter names and values, format as a string\n",
    "        parameters_with_names = f'{parameter_names[0]}: {parameters[0]:.6f}'\n",
    "        ax.set_xlabel(f'Parameters:\\n{parameters_with_names}')\n",
    "\n",
    "    for i in range(512, 8 * 4):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_waveforms(real_signals_batch, real_parameter_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on classifier\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.main = nn.Sequential(\n",
    "                nn.Conv1d(nc, ndf, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout2d(0.2),\n",
    "\n",
    "                nn.Conv1d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(ndf * 2),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout2d(0.2),\n",
    "\n",
    "                nn.Conv1d(ndf * 2, ndf * 4, kernel_size=4,\n",
    "                        stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(ndf * 4),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout2d(0.2),\n",
    "\n",
    "                nn.Conv1d(ndf * 4, ndf * 8, kernel_size=4,\n",
    "                        stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(ndf * 8),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout2d(0.2),\n",
    "\n",
    "                nn.Conv1d(ndf * 8, ndf * 16, kernel_size=4,\n",
    "                        stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(ndf * 16),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout2d(0.2),\n",
    "\n",
    "                nn.Conv1d(ndf * 16, ndf * 32, kernel_size=4,\n",
    "                        stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(ndf * 32),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout2d(0.2),\n",
    "\n",
    "                nn.Conv1d(ndf * 32, ndf * 64, kernel_size=4,\n",
    "                        stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(ndf * 64),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout2d(0.2),\n",
    "                \n",
    "                nn.Conv1d(ndf * 64, nc, kernel_size=4,\n",
    "                        stride=2, padding=0, bias=False)\n",
    "            )\n",
    "\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.main(x)\n",
    "            x = x.view(x.shape[0], -1)  # Flatten the tensor\n",
    "        #     x = self.fc(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv1d(1, 64, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Dropout2d(p=0.2, inplace=False)\n",
      "    (3): Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)\n",
      "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Dropout2d(p=0.2, inplace=False)\n",
      "    (7): Conv1d(128, 256, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)\n",
      "    (8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (10): Dropout2d(p=0.2, inplace=False)\n",
      "    (11): Conv1d(256, 512, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)\n",
      "    (12): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (14): Dropout2d(p=0.2, inplace=False)\n",
      "    (15): Conv1d(512, 1024, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)\n",
      "    (16): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (18): Dropout2d(p=0.2, inplace=False)\n",
      "    (19): Conv1d(1024, 2048, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)\n",
      "    (20): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (21): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (22): Dropout2d(p=0.2, inplace=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 64, 256]             256\n",
      "         LeakyReLU-2              [-1, 64, 256]               0\n",
      "         Dropout2d-3              [-1, 64, 256]               0\n",
      "            Conv1d-4             [-1, 128, 128]          32,768\n",
      "       BatchNorm1d-5             [-1, 128, 128]             256\n",
      "         LeakyReLU-6             [-1, 128, 128]               0\n",
      "         Dropout2d-7             [-1, 128, 128]               0\n",
      "            Conv1d-8              [-1, 256, 64]         131,072\n",
      "       BatchNorm1d-9              [-1, 256, 64]             512\n",
      "        LeakyReLU-10              [-1, 256, 64]               0\n",
      "        Dropout2d-11              [-1, 256, 64]               0\n",
      "           Conv1d-12              [-1, 512, 32]         524,288\n",
      "      BatchNorm1d-13              [-1, 512, 32]           1,024\n",
      "        LeakyReLU-14              [-1, 512, 32]               0\n",
      "        Dropout2d-15              [-1, 512, 32]               0\n",
      "           Conv1d-16             [-1, 1024, 16]       2,097,152\n",
      "      BatchNorm1d-17             [-1, 1024, 16]           2,048\n",
      "        LeakyReLU-18             [-1, 1024, 16]               0\n",
      "        Dropout2d-19             [-1, 1024, 16]               0\n",
      "           Conv1d-20              [-1, 2048, 8]       8,388,608\n",
      "      BatchNorm1d-21              [-1, 2048, 8]           4,096\n",
      "        LeakyReLU-22              [-1, 2048, 8]               0\n",
      "        Dropout2d-23              [-1, 2048, 8]               0\n",
      "================================================================\n",
      "Total params: 11,182,080\n",
      "Trainable params: 11,182,080\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 2.88\n",
      "Params size (MB): 42.66\n",
      "Estimated Total Size (MB): 45.53\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarineccleston/Documents/software-ds/gravitational-waves/gw-env/lib/python3.9/site-packages/torch/nn/functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
     ]
    }
   ],
   "source": [
    "net = Discriminator().to(device)\n",
    "net.apply(weights_init)\n",
    "\n",
    "print(net)\n",
    "\n",
    "model = Discriminator()\n",
    "summary(model, input_size=(1, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions and Optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "# maybe we need to use Adam\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarineccleston/Documents/software-ds/gravitational-waves/gw-env/lib/python3.9/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([32, 1, 1])) that is different to the input size (torch.Size([32, 16384])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   150] loss: 49.434\n",
      "[2,   150] loss: 41.914\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tarineccleston/Documents/software-ds/gravitational-waves/gw-classification/gw_classification.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tarineccleston/Documents/software-ds/gravitational-waves/gw-classification/gw_classification.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# if (i % 10 == 0):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tarineccleston/Documents/software-ds/gravitational-waves/gw-classification/gw_classification.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# print(outputs)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tarineccleston/Documents/software-ds/gravitational-waves/gw-classification/gw_classification.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# print(labels)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tarineccleston/Documents/software-ds/gravitational-waves/gw-classification/gw_classification.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tarineccleston/Documents/software-ds/gravitational-waves/gw-classification/gw_classification.ipynb#X23sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tarineccleston/Documents/software-ds/gravitational-waves/gw-classification/gw_classification.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tarineccleston/Documents/software-ds/gravitational-waves/gw-classification/gw_classification.ipynb#X23sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# print statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/software-ds/gravitational-waves/gw-env/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/software-ds/gravitational-waves/gw-env/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        # print(inputs)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        # if (i % 10 == 0):\n",
    "            # print(outputs)\n",
    "        # print(labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss:.3f}')\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './beta_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1057]],\n",
      "\n",
      "        [[0.0489]],\n",
      "\n",
      "        [[0.0091]],\n",
      "\n",
      "        [[0.1607]],\n",
      "\n",
      "        [[0.0098]],\n",
      "\n",
      "        [[0.0955]],\n",
      "\n",
      "        [[0.0088]],\n",
      "\n",
      "        [[0.2027]],\n",
      "\n",
      "        [[0.2142]],\n",
      "\n",
      "        [[0.0892]],\n",
      "\n",
      "        [[0.0056]],\n",
      "\n",
      "        [[0.1139]],\n",
      "\n",
      "        [[0.0699]],\n",
      "\n",
      "        [[0.1423]],\n",
      "\n",
      "        [[0.0916]],\n",
      "\n",
      "        [[0.0651]],\n",
      "\n",
      "        [[0.1023]],\n",
      "\n",
      "        [[0.0011]],\n",
      "\n",
      "        [[0.0221]],\n",
      "\n",
      "        [[0.0004]],\n",
      "\n",
      "        [[0.0043]],\n",
      "\n",
      "        [[0.0090]],\n",
      "\n",
      "        [[0.1128]],\n",
      "\n",
      "        [[0.1682]],\n",
      "\n",
      "        [[0.1175]],\n",
      "\n",
      "        [[0.1478]],\n",
      "\n",
      "        [[0.0377]],\n",
      "\n",
      "        [[0.1260]],\n",
      "\n",
      "        [[0.2008]],\n",
      "\n",
      "        [[0.1652]],\n",
      "\n",
      "        [[0.0994]],\n",
      "\n",
      "        [[0.0865]]])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(test_loader)\n",
    "data, labels = next(dataiter)\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Discriminator()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.6363e-01],\n",
      "        [-1.6052e-01],\n",
      "        [ 1.3917e-01],\n",
      "        [ 1.8887e-01],\n",
      "        [ 1.5317e-01],\n",
      "        [-6.4272e-02],\n",
      "        [ 2.3691e-01],\n",
      "        [ 2.8353e+00],\n",
      "        [ 4.6752e+00],\n",
      "        [-1.5200e-03],\n",
      "        [ 1.3408e-01],\n",
      "        [ 2.0715e-01],\n",
      "        [ 3.5033e-01],\n",
      "        [-1.5086e-01],\n",
      "        [ 1.8421e-01],\n",
      "        [ 3.2941e-01],\n",
      "        [ 3.6152e-02],\n",
      "        [-1.2134e-02],\n",
      "        [ 3.8112e-03],\n",
      "        [ 8.7788e-02],\n",
      "        [ 1.9323e-01],\n",
      "        [ 2.4253e-01],\n",
      "        [ 2.3393e-01],\n",
      "        [ 1.1936e-01],\n",
      "        [ 4.9952e-02],\n",
      "        [ 1.2283e-01],\n",
      "        [ 2.1826e-01],\n",
      "        [ 2.9500e-01],\n",
      "        [ 1.2201e-01],\n",
      "        [ 8.6775e-02],\n",
      "        [ 3.3613e-01],\n",
      "        [ 9.0746e-02]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "outputs = net(data)\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gw-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
